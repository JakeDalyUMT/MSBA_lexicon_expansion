{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment I will attempt to subset tweets by users who could reasonably be assumed to be fans of, or at least familiar with, the hip hop group Insane Clown Posse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Ensure NLTK resources are downloaded (if not already)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Tweet\n",
      "0  what the heck in this \"time out\" bullshit on t...\n",
      "1  electronic cigarettes | blu cigs | starter kit...\n",
      "2  @gastro_celtic blu stu still smoking those mor...\n",
      "3  @dosh22 @starscream_blu  the door. it's just v...\n",
      "4  fradi abito smoking slim fit fradi blu royal i...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv('vaping_tweets.csv')\n",
    "\n",
    "# To check the first few rows of your dataframe\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am starting with the terms \"ICP\" a commonly used acronym for the group, and \"whoop.\" \"Whoop Whoop\" is commonly used as a greeting or interjection within the community of ICP fans (often referred to as \"Juggalos\" or \"The Family\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                                     Tweet\n",
      "18      chef eddie huang and vaping brand blu are goin...\n",
      "75      @therealjuicyj @currensy_spitta @ovo40 @chrisd...\n",
      "610     the next dumbass who whines to me about how na...\n",
      "1222    @mysticphonk jus come over to do laundry we ca...\n",
      "2062    screwed around and got a nicotine addiction fr...\n",
      "...                                                   ...\n",
      "335248  @keigh_see @foulkesy1 @bamadan78 @angrygeek2 @...\n",
      "335547  i added a video to a @youtube playlist https:/...\n",
      "337617  we're not even supposed to rly be outside but ...\n",
      "337714  vapeshlurp whoop whoop!! ðŸ˜¤ðŸ¦ðŸ’¨ðŸ’¨   #aspire #beard...\n",
      "337787  federal government supplies dan andrews vic go...\n",
      "\n",
      "[411 rows x 1 columns]>\n"
     ]
    }
   ],
   "source": [
    "subset = df[df['Tweet'].str.contains('ICP|whoop', case=False, na=False)]\n",
    "\n",
    "# Display the subset of rows where \"Tweet\" contains either \"ICP\" or \"whoop\"\n",
    "print(subset.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the \"compare_corpora\" function I built for our group comparison assignment, modified to remove usernames in tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_corpora(corpus_1, corpus_2, num_words, ratio_cutoff):\n",
    "    \"\"\"\n",
    "    Compares two corpora and returns a dictionary with various statistics and comparison results.\n",
    "    \n",
    "    Parameters:\n",
    "    corpus_1 (str): The first corpus.\n",
    "    corpus_2 (str): The second corpus.\n",
    "    num_words (int): The number of top most frequent words to consider for comparison.\n",
    "    ratio_cutoff (float): The frequency cutoff for considering tokens in comparison.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary containing the following keys:\n",
    "        - \"one\": Statistics for corpus_1.\n",
    "        - \"two\": Statistics for corpus_2.\n",
    "        - \"one_vs_two\": Comparison of top words in corpus_1 to corpus_2.\n",
    "        - \"two_vs_one\": Comparison of top words in corpus_2 to corpus_1.\n",
    "    \"\"\"\n",
    "    \n",
    "    def preprocess_text(corpus):\n",
    "        # Tokenize by splitting on white space\n",
    "        tokens = word_tokenize(corpus.lower())  # Tokenize and lowercase\n",
    "\n",
    "        # Remove words that start with \"@\"\n",
    "        tokens = [word for word in tokens if not word.startswith('@')]\n",
    "\n",
    "        # Remove punctuation from the remaining tokens\n",
    "        tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]  # Remove punctuation\n",
    "\n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "        return filtered_tokens\n",
    "    \n",
    "    def calculate_corpus_statistics(tokens, num_words):\n",
    "        # Total number of tokens\n",
    "        tokens_count = len(tokens)\n",
    "        \n",
    "        # Number of unique tokens\n",
    "        unique_tokens_count = len(set(tokens))\n",
    "        \n",
    "        # Average token length\n",
    "        avg_token_length = sum(len(token) for token in tokens) / tokens_count if tokens_count > 0 else 0\n",
    "        \n",
    "        # Lexical diversity\n",
    "        lexical_diversity = unique_tokens_count / tokens_count if tokens_count > 0 else 0\n",
    "        \n",
    "        # Top most frequent words\n",
    "        top_words = Counter(tokens).most_common(num_words)\n",
    "        \n",
    "        return {\n",
    "            \"tokens\": tokens_count,\n",
    "            \"unique_tokens\": unique_tokens_count,\n",
    "            \"avg_token_length\": avg_token_length,\n",
    "            \"lexical_diversity\": lexical_diversity,\n",
    "            \"top_words\": top_words\n",
    "        }\n",
    "\n",
    "    def calculate_use_ratio(token_counts, total_tokens, ratio_cutoff):\n",
    "        # Calculate the use_ratio for tokens that occur at least `ratio_cutoff` times\n",
    "        use_ratios = {token: count / total_tokens for token, count in token_counts.items() if count >= ratio_cutoff}\n",
    "        return use_ratios\n",
    "\n",
    "    # Preprocess both corpora\n",
    "    tokens_1 = preprocess_text(corpus_1)\n",
    "    tokens_2 = preprocess_text(corpus_2)\n",
    "    \n",
    "    # Calculate statistics for both corpora\n",
    "    corpus_1_stats = calculate_corpus_statistics(tokens_1, num_words)\n",
    "    corpus_2_stats = calculate_corpus_statistics(tokens_2, num_words)\n",
    "    \n",
    "    # Calculate frequency of tokens in both corpora\n",
    "    token_counts_1 = Counter(tokens_1)\n",
    "    token_counts_2 = Counter(tokens_2)\n",
    "    \n",
    "    # Calculate use_ratio for tokens in each corpus that occur at least `ratio_cutoff` times\n",
    "    use_ratios_1 = calculate_use_ratio(token_counts_1, corpus_1_stats[\"tokens\"], ratio_cutoff)\n",
    "    use_ratios_2 = calculate_use_ratio(token_counts_2, corpus_2_stats[\"tokens\"], ratio_cutoff)\n",
    "    \n",
    "    # Create dictionaries for 1v2 and 2v1\n",
    "    ratio_1v2 = {}\n",
    "    ratio_2v1 = {}\n",
    "\n",
    "    # Calculate the use_ratio comparison for each corpus\n",
    "    for token, use_ratio in use_ratios_1.items():\n",
    "        if token in use_ratios_2:\n",
    "            ratio_1v2[token] = use_ratio / use_ratios_2[token]\n",
    "\n",
    "    for token, use_ratio in use_ratios_2.items():\n",
    "        if token in use_ratios_1:\n",
    "            ratio_2v1[token] = use_ratio / use_ratios_1[token]\n",
    "\n",
    "    # Sort the ratios from highest to lowest\n",
    "    sorted_ratio_1v2 = sorted(ratio_1v2.items(), key=lambda x: x[1], reverse=True)\n",
    "    sorted_ratio_2v1 = sorted(ratio_2v1.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "    # Store the top `num_words` words with the highest ratios in the dictionaries using OrderedDict\n",
    "    dict_1v2 = OrderedDict(sorted_ratio_1v2[:num_words])\n",
    "    dict_2v1 = OrderedDict(sorted_ratio_2v1[:num_words])\n",
    "\n",
    "    # Create the final dictionary with the specified structure\n",
    "    result_dict = {\n",
    "        \"one\": corpus_1_stats,\n",
    "        \"two\": corpus_2_stats,\n",
    "        \"one_vs_two\": dict_1v2,\n",
    "        \"two_vs_one\": dict_2v1\n",
    "    }\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is my initial comparison and result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'one': {'avg_token_length': 5.889001888711317,\n",
      "         'lexical_diversity': 0.2888275461281418,\n",
      "         'tokens': 13766,\n",
      "         'top_words': [('', 2854),\n",
      "                       ('https', 214),\n",
      "                       ('juul', 168),\n",
      "                       ('smoking', 136),\n",
      "                       ('vape', 80),\n",
      "                       ('nicotine', 73),\n",
      "                       ('vaping', 71),\n",
      "                       ('doo', 61),\n",
      "                       ('get', 52),\n",
      "                       ('amp', 44)],\n",
      "         'unique_tokens': 3976},\n",
      " 'one_vs_two': OrderedDict([('kayzomusic', 2614.4375635624006),\n",
      "                            ('craysounds_', 2614.4375635624006),\n",
      "                            ('quackhouse', 2614.4375635624006),\n",
      "                            ('leespielman', 2556.338951038791),\n",
      "                            ('keigh_see', 2033.4514383263113),\n",
      "                            ('hi_mija', 1742.9583757082669),\n",
      "                            ('fantasticradiouk', 1646.1273548355853),\n",
      "                            ('dillonfrancis', 1510.563925613831),\n",
      "                            ('tcohhwlmdbgnl', 1510.563925613831),\n",
      "                            ('ekalimusic', 1379.8420474357113)]),\n",
      " 'two': {'avg_token_length': 4.649715579989885,\n",
      "         'lexical_diversity': 0.05284316857457406,\n",
      "         'tokens': 7997855,\n",
      "         'top_words': [('', 1925561),\n",
      "                       ('https', 213105),\n",
      "                       ('juul', 180014),\n",
      "                       ('vape', 96053),\n",
      "                       ('vaping', 90712),\n",
      "                       ('smoking', 85549),\n",
      "                       ('nicotine', 56991),\n",
      "                       ('cigarettes', 41936),\n",
      "                       ('like', 36347),\n",
      "                       ('new', 35535)],\n",
      "         'unique_tokens': 422632},\n",
      " 'two_vs_one': OrderedDict([('aspire', 5.182854694581652),\n",
      "                            ('innokin', 3.5491381126564563),\n",
      "                            ('tank', 3.284071541682114),\n",
      "                            ('new', 3.0581625323289807),\n",
      "                            ('savage', 2.98492498301107),\n",
      "                            ('quit', 2.7757977857813123),\n",
      "                            ('addicted', 2.408319130566883),\n",
      "                            ('cigs', 2.3587482393716814),\n",
      "                            ('vapes', 2.297473109977613),\n",
      "                            ('vaping', 2.19907799398432)])}\n"
     ]
    }
   ],
   "source": [
    "# Separate the rows in df that are not in subset\n",
    "non_subset = df[~df.index.isin(subset.index)]\n",
    "\n",
    "# Combine the \"Tweet\" text into strings for each corpus\n",
    "corpus_1 = \" \".join(subset[\"Tweet\"].tolist())\n",
    "corpus_2 = \" \".join(non_subset[\"Tweet\"].tolist())\n",
    "\n",
    "# Use your compare_corpora function to compare the corpora\n",
    "result = compare_corpora(corpus_1, corpus_2, num_words=10, ratio_cutoff=5)\n",
    "\n",
    "# Print the comparison result\n",
    "pprint(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will build a new subset incorporating three words with unusually high usage ratios in the first subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the first subset with rows containing any of the specified keywords\n",
    "subset_with_keywords = df[\n",
    "    df['Tweet'].str.contains('ICP|whoop|kayzomusic|craysounds_|quackhouse|leespielman', case=False, na=False)\n",
    "]\n",
    "\n",
    "# Create the second subset with rows that do not contain the specified keywords\n",
    "subset_without_keywords = df[\n",
    "    ~df['Tweet'].str.contains('ICP|whoop|kayzomusic|craysounds_|quackhouse|leespielman', case=False, na=False)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run the compare corpora function on my new subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'one': {'avg_token_length': 5.940816756870738,\n",
      "         'lexical_diversity': 0.28270190482884655,\n",
      "         'tokens': 14227,\n",
      "         'top_words': [('', 2892),\n",
      "                       ('https', 214),\n",
      "                       ('juul', 177),\n",
      "                       ('smoking', 137),\n",
      "                       ('vape', 85),\n",
      "                       ('nicotine', 79),\n",
      "                       ('vaping', 73),\n",
      "                       ('doo', 61),\n",
      "                       ('get', 53),\n",
      "                       ('amp', 49)],\n",
      "         'unique_tokens': 4022},\n",
      " 'one_vs_two': OrderedDict([('diplo', 2569.7276606854175),\n",
      "                            ('kidcudi', 2248.51170309974),\n",
      "                            ('keigh_see', 1967.4477402122723),\n",
      "                            ('garethemery', 1798.8093624797923),\n",
      "                            ('arminvanbuuren', 1798.8093624797923),\n",
      "                            ('ashwallbridge', 1798.8093624797923),\n",
      "                            ('ekalimusic', 1606.0797879283857),\n",
      "                            ('fantasticradiouk', 1592.695789695649),\n",
      "                            ('tcohhwlmdbgnl', 1461.532607014831),\n",
      "                            ('illgatesmusic', 1383.69950959984)]),\n",
      " 'two': {'avg_token_length': 4.649551966553105,\n",
      "         'lexical_diversity': 0.05284496424710349,\n",
      "         'tokens': 7997394,\n",
      "         'top_words': [('', 1925523),\n",
      "                       ('https', 213105),\n",
      "                       ('juul', 180005),\n",
      "                       ('vape', 96048),\n",
      "                       ('vaping', 90710),\n",
      "                       ('smoking', 85548),\n",
      "                       ('nicotine', 56985),\n",
      "                       ('cigarettes', 41935),\n",
      "                       ('like', 36344),\n",
      "                       ('new', 35535)],\n",
      "         'unique_tokens': 422622},\n",
      " 'two_vs_one': OrderedDict([('aspire', 4.591227413758603),\n",
      "                            ('innokin', 3.6682041675075663),\n",
      "                            ('tank', 3.394245175365875),\n",
      "                            ('new', 3.1607573979723895),\n",
      "                            ('savage', 3.085062884234539),\n",
      "                            ('health', 2.8352976732170503),\n",
      "                            ('addicted', 2.489113128601642),\n",
      "                            ('cigs', 2.437879239162157),\n",
      "                            ('quit', 2.3904701018856893),\n",
      "                            ('vapes', 2.3745484591605717)])}\n"
     ]
    }
   ],
   "source": [
    "# Combine the \"Tweet\" text from each subset into a single string\n",
    "corpus_3 = \" \".join(subset_with_keywords[\"Tweet\"].tolist())\n",
    "corpus_4 = \" \".join(subset_without_keywords[\"Tweet\"].tolist())\n",
    "\n",
    "# Run the compare_corpora function on the two corpora\n",
    "result = compare_corpora(corpus_3, corpus_4, num_words=10, ratio_cutoff=5)\n",
    "\n",
    "# Print the comparison result\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was so much fun! Let's do it again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the first subset with rows containing any of the specified keywords\n",
    "subset_with_2 = df[\n",
    "    df['Tweet'].str.contains('ICP|whoop|kayzomusic|craysounds_|quackhouse|leespielman|diplo|kidcudi', case=False, na=False)\n",
    "]\n",
    "\n",
    "# Create the second subset with rows that do not contain the specified keywords\n",
    "subset_without_2 = df[\n",
    "    ~df['Tweet'].str.contains('ICP|whoop|kayzomusic|craysounds_|quackhouse|leespielman|diplo|kidcudi', case=False, na=False)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'one': {'avg_token_length': 5.9903119550474715,\n",
      "         'lexical_diversity': 0.27669056384421625,\n",
      "         'tokens': 15483,\n",
      "         'top_words': [('', 3061),\n",
      "                       ('https', 222),\n",
      "                       ('juul', 186),\n",
      "                       ('smoking', 151),\n",
      "                       ('vape', 85),\n",
      "                       ('nicotine', 82),\n",
      "                       ('vaping', 77),\n",
      "                       ('doo', 61),\n",
      "                       ('amp', 58),\n",
      "                       ('get', 56)],\n",
      "         'unique_tokens': 4284},\n",
      " 'one_vs_two': OrderedDict([('illgatesmusic', 4131.5703674998385),\n",
      "                            ('ovo40', 1877.9865306817446),\n",
      "                            ('asvpxrocky', 1877.9865306817446),\n",
      "                            ('keigh_see', 1807.5620357811795),\n",
      "                            ('theslumpgod', 1589.0655259614764),\n",
      "                            ('madeintyo', 1589.0655259614764),\n",
      "                            ('2chainz', 1475.5608455356564),\n",
      "                            ('ekalimusic', 1475.5608455356564),\n",
      "                            ('xavierwulf', 1475.5608455356564),\n",
      "                            ('fantasticradiouk', 1463.264505156193)]),\n",
      " 'two': {'avg_token_length': 4.649253302031556,\n",
      "         'lexical_diversity': 0.05284576129126336,\n",
      "         'tokens': 7996138,\n",
      "         'top_words': [('', 1925354),\n",
      "                       ('https', 213097),\n",
      "                       ('juul', 179996),\n",
      "                       ('vape', 96048),\n",
      "                       ('vaping', 90706),\n",
      "                       ('smoking', 85534),\n",
      "                       ('nicotine', 56982),\n",
      "                       ('cigarettes', 41934),\n",
      "                       ('like', 36339),\n",
      "                       ('new', 35535)],\n",
      "         'unique_tokens': 422562},\n",
      " 'two_vs_one': OrderedDict([('aspire', 4.997338858190935),\n",
      "                            ('innokin', 3.992670711786115),\n",
      "                            ('tank', 3.6944790097419533),\n",
      "                            ('new', 3.440338354590679),\n",
      "                            ('health', 3.0860904851817215),\n",
      "                            ('savage', 2.7979675938559345),\n",
      "                            ('addicted', 2.709284607144099),\n",
      "                            ('cigs', 2.653518886242334),\n",
      "                            ('vapes', 2.58458625901654),\n",
      "                            ('flavors', 2.4563472301968057)])}\n"
     ]
    }
   ],
   "source": [
    "# Combine the \"Tweet\" text from each subset into a single string\n",
    "corpus_5 = \" \".join(subset_with_2[\"Tweet\"].tolist())\n",
    "corpus_6 = \" \".join(subset_without_2[\"Tweet\"].tolist())\n",
    "\n",
    "# Run the compare_corpora function on the two corpora\n",
    "result = compare_corpora(corpus_5, corpus_6, num_words=10, ratio_cutoff=5)\n",
    "\n",
    "# Print the comparison result\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ONE MORE TIME!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the first subset with rows containing any of the specified keywords\n",
    "subset_with_3 = df[\n",
    "    df['Tweet'].str.contains('ICP|whoop|kayzomusic|craysounds_|quackhouse|leespielman|diplo|kidcudi|illgatesmusic|ovo40|asvpxrocky|keigh_see', case=False, na=False)\n",
    "]\n",
    "\n",
    "# Create the second subset with rows that do not contain the specified keywords\n",
    "subset_without_3 = df[\n",
    "    ~df['Tweet'].str.contains('ICP|whoop|kayzomusic|craysounds_|quackhouse|leespielman|diplo|kidcudi|illgatesmusic|ovo40|asvpxrocky|keigh_see', case=False, na=False)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'one': {'avg_token_length': 6.143236875490848,\n",
      "         'lexical_diversity': 0.2701020962967438,\n",
      "         'tokens': 16553,\n",
      "         'top_words': [('', 3125),\n",
      "                       ('https', 224),\n",
      "                       ('juul', 202),\n",
      "                       ('smoking', 158),\n",
      "                       ('nicotine', 90),\n",
      "                       ('vape', 90),\n",
      "                       ('vaping', 77),\n",
      "                       ('amp', 66),\n",
      "                       ('doo', 61),\n",
      "                       ('get', 58)],\n",
      "         'unique_tokens': 4471},\n",
      " 'one_vs_two': OrderedDict([('devonsmillie', 4636.782021385851),\n",
      "                            ('tonykay69', 4636.782021385851),\n",
      "                            ('chadkerley', 3944.484705692825),\n",
      "                            ('richbrian', 3477.5865160393887),\n",
      "                            ('chrisdelia', 2958.3635292696185),\n",
      "                            ('hicksfilmedit', 2511.590261584003),\n",
      "                            ('ibluestone', 2221.791385247387),\n",
      "                            ('lexx_flame', 1770.9931331682071),\n",
      "                            ('getterofficial', 1750.8682112003867),\n",
      "                            ('kreayshawn', 1738.7932580196943)]),\n",
      " 'two': {'avg_token_length': 4.648757208819237,\n",
      "         'lexical_diversity': 0.05283694897904558,\n",
      "         'tokens': 7995068,\n",
      "         'top_words': [('', 1925290),\n",
      "                       ('https', 213095),\n",
      "                       ('juul', 179980),\n",
      "                       ('vape', 96043),\n",
      "                       ('vaping', 90706),\n",
      "                       ('smoking', 85527),\n",
      "                       ('nicotine', 56974),\n",
      "                       ('cigarettes', 41933),\n",
      "                       ('like', 36335),\n",
      "                       ('new', 35534)],\n",
      "         'unique_tokens': 422435},\n",
      " 'two_vs_one': OrderedDict([('aspire', 5.343410248132259),\n",
      "                            ('innokin', 4.269167691882045),\n",
      "                            ('tank', 3.950325875902494),\n",
      "                            ('new', 3.503316354056085),\n",
      "                            ('health', 3.299805755248111),\n",
      "                            ('savage', 2.991730026561375),\n",
      "                            ('ejuice', 2.5623287756902133),\n",
      "                            ('kit', 2.4929358220176074),\n",
      "                            ('product', 2.453011581640081),\n",
      "                            ('vaping', 2.4389328521033216)])}\n"
     ]
    }
   ],
   "source": [
    "# Combine the \"Tweet\" text from each subset into a single string\n",
    "corpus_7 = \" \".join(subset_with_3[\"Tweet\"].tolist())\n",
    "corpus_8 = \" \".join(subset_without_3[\"Tweet\"].tolist())\n",
    "\n",
    "# Run the compare_corpora function on the two corpora\n",
    "result = compare_corpora(corpus_7, corpus_8, num_words=10, ratio_cutoff=5)\n",
    "\n",
    "# Print the comparison result\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I appear to have unintentionally built an \"Other artists you might like\" function... Let's see if it works with a different group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'one': {'avg_token_length': 4.0064,\n",
      "         'lexical_diversity': 0.3792,\n",
      "         'tokens': 2500,\n",
      "         'top_words': [('', 684),\n",
      "                       ('migos', 64),\n",
      "                       ('katy', 36),\n",
      "                       ('perry', 36),\n",
      "                       ('savage', 36),\n",
      "                       ('https', 34),\n",
      "                       ('21', 31),\n",
      "                       ('amp', 22),\n",
      "                       ('lil', 22),\n",
      "                       ('logic', 15)],\n",
      "         'unique_tokens': 948},\n",
      " 'one_vs_two': OrderedDict([('vert', 1348.9047578947368),\n",
      "                            ('uzi', 743.7041857142857),\n",
      "                            ('lamar', 431.2604153846154),\n",
      "                            ('travis', 374.45245714285716),\n",
      "                            ('malone', 348.9122455445545),\n",
      "                            ('kendrick', 251.9723775280899),\n",
      "                            ('drake', 240.27366),\n",
      "                            ('cole', 213.57658666666666),\n",
      "                            ('khalid', 186.6203184466019),\n",
      "                            ('cardi', 157.55649836065572)]),\n",
      " 'two': {'avg_token_length': 4.652045879685688,\n",
      "         'lexical_diversity': 0.052852235238768995,\n",
      "         'tokens': 8009122,\n",
      "         'top_words': [('', 1927732),\n",
      "                       ('https', 213285),\n",
      "                       ('juul', 180181),\n",
      "                       ('vape', 96125),\n",
      "                       ('vaping', 90783),\n",
      "                       ('smoking', 85674),\n",
      "                       ('nicotine', 57064),\n",
      "                       ('cigarettes', 41972),\n",
      "                       ('like', 36377),\n",
      "                       ('new', 35547)],\n",
      "         'unique_tokens': 423300},\n",
      " 'two_vs_one': OrderedDict([('vape', 3.7506061837989235),\n",
      "                            ('smoking', 2.431148337653445),\n",
      "                            ('https', 1.9581073416331147),\n",
      "                            ('cigarettes', 1.8716158899814486),\n",
      "                            ('new', 1.386973191318599),\n",
      "                            ('one', 1.3026396651218446),\n",
      "                            ('like', 1.2616516794502846),\n",
      "                            ('', 0.8797224082199336),\n",
      "                            ('logic', 0.5989004371099187),\n",
      "                            ('1', 0.530288196027337)])}\n"
     ]
    }
   ],
   "source": [
    "# Create the first subset with rows containing any of the specified keywords\n",
    "subset_with = df[\n",
    "    df['Tweet'].str.contains('Migos', case=False, na=False)\n",
    "]\n",
    "\n",
    "# Create the second subset with rows that do not contain the specified keywords\n",
    "subset_without = df[\n",
    "    ~df['Tweet'].str.contains('Migos', case=False, na=False)\n",
    "]\n",
    "# Combine the \"Tweet\" text from each subset into a single string\n",
    "corpus_9 = \" \".join(subset_with[\"Tweet\"].tolist())\n",
    "corpus_10 = \" \".join(subset_without[\"Tweet\"].tolist())\n",
    "\n",
    "# Run the compare_corpora function on the two corpora\n",
    "result = compare_corpora(corpus_9, corpus_10, num_words=10, ratio_cutoff=5)\n",
    "\n",
    "# Print the comparison result\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relatively strong in the \"artist suggestion\" department. \"Uzi\" and \"Vert\" refer to a member of migos. \"Lamar\" and \"kendrick\" could refer to popular hip hop artist kendrick lamar. Other possible artist mentions are Travis Scott, Post Malone, Drake, J. Cole, DJ Khalid, and Cardi B. Lets see what one more iteration returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'one': {'avg_token_length': 5.205376380861178,\n",
      "         'lexical_diversity': 0.20065665720441875,\n",
      "         'tokens': 58478,\n",
      "         'top_words': [('', 11360),\n",
      "                       ('juul', 709),\n",
      "                       ('smoking', 569),\n",
      "                       ('https', 566),\n",
      "                       ('logic', 462),\n",
      "                       ('vape', 372),\n",
      "                       ('katy', 331),\n",
      "                       ('perry', 321),\n",
      "                       ('x', 284),\n",
      "                       ('cigarettes', 283)],\n",
      "         'unique_tokens': 11734},\n",
      " 'one_vs_two': OrderedDict([('johnferris62', 761.6133109887479),\n",
      "                            ('ruth91869132', 646.0112905708129),\n",
      "                            ('pmatist', 639.2111717226992),\n",
      "                            ('imbalaska', 639.2111717226992),\n",
      "                            ('bartbow', 634.67775915729),\n",
      "                            ('iandjbrown2', 600.6771649167209),\n",
      "                            ('sander_1954', 600.6771649167209),\n",
      "                            ('arthurbraby', 600.6771649167209),\n",
      "                            ('guygadboisguyg1', 600.6771649167209),\n",
      "                            ('sniemn', 598.4104586340163)]),\n",
      " 'two': {'avg_token_length': 4.647772636416755,\n",
      "         'lexical_diversity': 0.05282776742338599,\n",
      "         'tokens': 7953147,\n",
      "         'top_words': [('', 1917059),\n",
      "                       ('https', 212753),\n",
      "                       ('juul', 179473),\n",
      "                       ('vape', 95761),\n",
      "                       ('vaping', 90512),\n",
      "                       ('smoking', 85116),\n",
      "                       ('nicotine', 56817),\n",
      "                       ('cigarettes', 41696),\n",
      "                       ('like', 36180),\n",
      "                       ('new', 35471)],\n",
      "         'unique_tokens': 420147},\n",
      " 'two_vs_one': OrderedDict([('kit', 15.182507754477568),\n",
      "                            ('vapefam', 12.330666841691723),\n",
      "                            ('win', 7.9581942426899275),\n",
      "                            ('aspire', 7.8095520885890695),\n",
      "                            ('review', 7.224873726086038),\n",
      "                            ('pro', 5.333940388628552),\n",
      "                            ('epidemic', 5.145743397760241),\n",
      "                            ('labs', 4.851385797345378),\n",
      "                            ('ecig', 4.826541030335473),\n",
      "                            ('hit', 4.656335730429042)])}\n"
     ]
    }
   ],
   "source": [
    "# Create the first subset with rows containing any of the specified keywords\n",
    "subset_with = df[\n",
    "    df['Tweet'].str.contains('Migos|lamar|travis|malone|kendrick|drake|cole|khalid|cardi', case=False, na=False)\n",
    "]\n",
    "\n",
    "# Create the second subset with rows that do not contain the specified keywords\n",
    "subset_without = df[\n",
    "    ~df['Tweet'].str.contains('Migos|lamar|travis|malone|kendrick|drake|cole|khalid|cardi', case=False, na=False)\n",
    "]\n",
    "# Combine the \"Tweet\" text from each subset into a single string\n",
    "corpus_9 = \" \".join(subset_with[\"Tweet\"].tolist())\n",
    "corpus_10 = \" \".join(subset_without[\"Tweet\"].tolist())\n",
    "\n",
    "# Run the compare_corpora function on the two corpora\n",
    "result = compare_corpora(corpus_9, corpus_10, num_words=10, ratio_cutoff=5)\n",
    "\n",
    "# Print the comparison result\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a very strong slate of music suggestions. This iteration appears to return all usernames of people mentioned in tweets involving the artists mentioned above. Variations on the \"compare_corpora\" functioned outlined here could possibly be used to suggest new artists or identify other users with similar music tastes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
